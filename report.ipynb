{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS 542 Midterm Challenge Report\n",
    "Ann Liang <br>\n",
    "March 30, 2025 <br>\n",
    "Kaggle User Name: aaaaaliang <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AI Disclosure Statement \n",
    "This machine learning challenge was completed using a Pretrained CNN model (ResNet50) and an AI tool (ChatGPT). The following list details how ChatGPT was used throughout the completion of this challenge: \n",
    "- Model architecture comparison, selection, recommendation of adjusting the number of convolutional layers and using ResNet18 and Resnet50 with the technique of unfreezing the latter layers. \n",
    "- Troubleshoot of SCC set up, create batch jobs, write shell scripts, and define best GPU usage. \n",
    "- Data augmentation strategies recommendation of using random horizontal flip, color jitter, and random erasing. \n",
    "- Hyperparameter tuning strategies recommendation of adjusting optimizer, learning rate scheduler, and weight decay. \n",
    "- Use of WandB for experiment tracking. \n",
    "- Organize notes on experiment observations and help interpret results for final report. \n",
    "1. Written by Me: \n",
    "- Basic structure of CNN models, training and testing loop, data loading, model initiation, loss function, optimizer, and scheduler\n",
    "- Basic structure of batch job shell scripts \n",
    "- Basic data transformation steps, such as random rotation, random cropping, and random normalization \n",
    "2. Written with AI Assistance: \n",
    "- Detailed structure of ResNet18 and ResNet50 functions. \n",
    "- Refined data augmentation recommendation on random horizontal flip, color jitter, and random erasing. \n",
    "- Refined hyperparameter tuning recommendation on optimizer, learning rate scheduler, and weight decay. \n",
    "- Code organization and CUDA setting for SCC adoption. \n",
    "- Organize notes on experiment observations and help interpret results for final report. \n",
    "3. Code Comment: \n",
    "- All codes are noted with detailed comments in their respective .py files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Description and Justification\n",
    "There are three primary CNN models implemented: \n",
    "1. **Simple CNN**: \n",
    "- Architecture: A convolutional neural network with four layers. \n",
    "- Why Simple CNN? To provide an initial benchmark performance for comparison. \n",
    "3. **Sophisticated CNN (ResNet18)**: \n",
    "- Architecture: A more refined convolutional neural network using ResNet18 with residual network and skip connections. \n",
    "- Why ResNet18? To better extract features and improve model performance from Simple CNN. \n",
    "4. **Pretrained CNN (ResNet50)**: \n",
    "- Architecture: A convolutional neural network pretrained on ImageNet using ResNet50 with deeper residual network. \n",
    "-  Why ResNet50? To better fine-tune on CIFAR-100 dataset for the best model performance with the following advantages: \n",
    "    - Deeper network can capture more complicated patterns.\n",
    "    - Pretrained model has stronger feature extraction strength. \n",
    "    - Unfreezing the latter layers adapts to CIFAR-100 faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result Analysis \n",
    "\n",
    "1. **Simple CNN (Ecophs=30)**: \n",
    "- Optimizer: SGD \n",
    "- Batch Size: 32 \n",
    "- Learning Rate: 0.01 \n",
    "- Ecoph: 30 \n",
    "- Test Accuracy: 61.16% \n",
    "- Kaggle Accuracy: 45.23% (File: submission_ood (3).csv)\n",
    "- Analysis: The Simple CNN model established baseline performance, but the number of ecoph was not as instructed on Kaggle and was re-created as below. The CIFAR-100 test accuracy was much higher than Kaggle showing that the model generalization ability was weak. \n",
    "\n",
    "2. **Simple CNN (Ecophs=5)**: \n",
    "- Optimizer: SGD \n",
    "- Batch Size: 32 \n",
    "- Learning Rate: 0.01 \n",
    "- Ecoph: 5 \n",
    "- Test Accuracy: 32.89%\n",
    "- Kaggle Accuracy: 24.80% (File: submission_ood_simple_cnn.csv)\n",
    "- Analysis: After adjusting the number of ecoph from 30 to 5, the baseline performance decreased from 45.23% to 24.80%. This emphasizes the number of ecophs impact the number of times the model learns from the dataset, and thus, the performance.  \n",
    "\n",
    "3. **Sophisticated CNN (ResNet18)**: \n",
    "- Optimizer: SGD \n",
    "- Batch Size: 32 \n",
    "- Learning Rate: 0.01 \n",
    "- Ecoph: 5 \n",
    "- Test Accuracy: 35.24% \n",
    "- Kaggle Accuracy: 29.72% (File: submission_ood_soph_cnn (1).csv)\n",
    "- Analysis: After traning with ResNet18, the performance was improved with an increased accuracy of ~5%. This highlights the advantage of training deeper networks and creating better performance and the opportunity for further improvement. \n",
    "\n",
    "4. **Pretrained CNN (ResNet50)**: \n",
    "- Optimizer: SGD \n",
    "- Batch Size: 64 \n",
    "- Learning Rate: 0.003 \n",
    "- Ecoph: 50 \n",
    "- Test Accuracy: 56.98% \n",
    "- Kaggle Accuracy: 44.95% (File: submission_ood_pretrained_cnn.csv)\n",
    "- Analysis: After training with ResNet50, the performance was more improved with an increased accuracy of ~15%. This shows that ResNet50 has even deeper networks for training and can capture more features. Although the model has slightly lower Kaggle accuracy than Simple CNN (30 epochs), it generalized better with a lower accuracy gap between test and Kaggle (12% vs. 16%). \n",
    "\n",
    "Since the Pretrained CNN (ResNet50) has the best overall balance, the following sections focus on explaining its design. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "Based on prior results, hyperparameter tuning was performed to experiment with the following parameters and establish the optimal model. Below highlights the change from Sophisticated CNN (ResNet18) to Pretrained CNN (ResNet50). \n",
    "- **Optimizer**: Changed from SGD to AdamW for better L2 regularization and adaptive learning rates.\n",
    "- **Batch Size**: Increased from 32 to 64 for better gradient estimates. \n",
    "- **Learning Rate**: Decreased from 0.01 to 0.003 to avoid over-estimating weights.\n",
    "- **Learning Rate Scheduler**: Changed StepLR from to CosineAnnealingLR to gradually reduce learning rate. \n",
    "- **Ecoph**: Define as 50 to form better fine-tuning for model. \n",
    "- **Weight Decay**: Increased from 5e-4 to 2e-4 to balance learning capacity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization Techniques\n",
    "Regularization techniques were incorporated to reduce model complexity and increase generalization. \n",
    "- **Weight Decay**: It stabilizes training by preventing the weights from getting too large. It increases the model performance. \n",
    "- **Label Smoothing**: It improves test accuracy by eliminating noises from dataset. It improves model robutness. \n",
    "- **Dropout**: It deactivates neurons in training by preventing overfitting and improving generalization. It handles model batch normalization layers better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation Strategy\n",
    "Data augmentation was used to generate random variations, prevent over-fitting, and improve model generalization. The key strategies include: \n",
    "- **To Tensor**: It converts images to tensors to allow batch processing in model. \n",
    "- **Random Horizontal Flip**: It creates more variation in image orientations. \n",
    "- **Random Rotation**: It exposes model to a wider range of image orientations. \n",
    "- **Color Jitter**: It changes brightness, contrast, and saturation of the images. \n",
    "- **Random Cropping with Padding**: It exposes model to various image compositions. \n",
    "- **Random Erasing**: It forces model to learn by erasing some portions of the images. \n",
    "- **Normalize**: It standardizes image inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Tracking Summary\n",
    "Weights and Biases (WandB) was used to track the data of training and validation sets of each model run. Below is the dashboard of successful model runs, excluding failed and crashed runs from setting up at an early stage: \n",
    "- **Training and Validation Loss**: An ideal model should have both losses decreasing and remaining low. Some models show this pattern, meaning they are learning well. But, some models have validation loss that remains high or fluctuates, meaning they are not generalizing well to data. \n",
    "- **Training and Validation Accuracy**: An ideal model should have both accuracies increase together. Some models show good progress, while some might be overfitting. \n",
    "\n",
    "![Image](wandb_tracking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Areas for Improvement \n",
    "The following aspects can be addressed to further improve CNN models:  \n",
    "- Experiment with larger bath sizes. \n",
    "- Fine-tune more layers and epochs to increase accuracy. \n",
    "- Try out different  optimizers and schedulers.\n",
    "- Explore other hyperparameter tuning, regularization techniques, and data augmentation strategies for higher model accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "This challenge reinforced the effectiveness of using a pretrained CNN model, such as ResNet50, over a more simple CNN model. By adjusting hyperparameters and data augmentation strategies, the final accuracy score achieved considerable improvements in both the CIFAR-100 test and Kaggle performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
